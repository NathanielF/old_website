<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Randomly Jittered</title>
    <link>https://NathanielF.github.io/</link>
    <description>Recent content on Randomly Jittered</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2017 Nathaniel Forde</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://NathanielF.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Test</title>
      <link>https://NathanielF.github.io/post/test/</link>
      <pubDate>Sun, 15 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://NathanielF.github.io/post/test/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://NathanielF.github.io/portfolio/neuralnet/</link>
      <pubDate>Fri, 26 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/neuralnet/</guid>
      <description>The clear limitations of the perceptron prompted criticism of the connectionist paradigm in machine learning. In 1969 Marvin Minsky released a scathing critique of the prospects for this approah elaborating a series of examples which could not (even in principle) be learned by the perceptron. Notably the concept of exclusive disjunction could not be captured by any single perceptron.
In part this limitation stems from the fact that the Heaviside activation function forces us to have a discrete delta between our prediction and target.</description>
    </item>
    
    <item>
      <title>The KNN algorithm</title>
      <link>https://NathanielF.github.io/blog/knn/</link>
      <pubDate>Fri, 26 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/blog/knn/</guid>
      <description>In this post we will implement the K-Nearest Neighbour classification algorithm. The idea is simply stated that &amp;ldquo;we are the company we keep&amp;rdquo;. The algorithm surveys an entire known population and compares each candidate to that population to determine where (along a series of metrics) that candidate best fits. Once we have ascertained a measure of fit we further identify the candidate with the most common class of the members of the population nearest to them in the population.</description>
    </item>
    
    <item>
      <title>The KNN algorithm</title>
      <link>https://NathanielF.github.io/portfolio/knn/</link>
      <pubDate>Fri, 26 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/knn/</guid>
      <description>In this post we will implement the K-Nearest Neighbour classification algorithm. The idea is simply stated that &amp;ldquo;we are the company we keep&amp;rdquo;. The algorithm surveys an entire known population and compares each candidate to that population to determine where (along a series of metrics) that candidate best fits. Once we have ascertained a measure of fit we further identify the candidate with the most common class of the members of the population nearest to them in the population.</description>
    </item>
    
    <item>
      <title>Matrices and Machine Learning</title>
      <link>https://NathanielF.github.io/portfolio/matrixmultiplication/</link>
      <pubDate>Mon, 08 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/matrixmultiplication/</guid>
      <description>The ubiquity of matrix manipulation in machine learning means that languages like R and Python, which have been optimised for linear algebra, are very well suited to the task of making robots think. It is less clear to me whether Skynet could be written in Golang?
There are a number of machine learning packages in development but none have the naturalness of an R implementation. To that end I decided to create some of the functionality I need.</description>
    </item>
    
    <item>
      <title>Perceptron</title>
      <link>https://NathanielF.github.io/portfolio/perceptron/perceptron/</link>
      <pubDate>Mon, 08 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/perceptron/perceptron/</guid>
      <description>Despite sounding like the protagonist of rejected sci-fi script, the perceptron is just a lowly algorithm. Inspired by the model of neuronal triggering patterns, this simple linear classifier was designed to enact a pseudo-biological processing mechanism. As our brain receives and processes different inputs it unconsciously corrects for infelicities in rendering based on a weighting derived from past experience. The thought is that we are progressively conditioned for behaviour that expedites performance of perceptual and physical reflex.</description>
    </item>
    
    <item>
      <title>Why is OLS BLUE?</title>
      <link>https://NathanielF.github.io/blog/ols-and-the-gauss-markov-theorem/</link>
      <pubDate>Mon, 08 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/blog/ols-and-the-gauss-markov-theorem/</guid>
      <description>There is an aesthetic discipline to mathematics as much as there is an expectation of rigour. We shall attempt to showcase the relation between aesthetic dimension of a proof with its explanatory purchase. We shall take as our example a standard proof used in regression analysis to find the slope of a line/plane which maps the trajectory of a series of observed points/locations along a trajectory. Given this problem we can draw a line/plane with the slope and it can be shown that this line (as a function of slope) is the best available line that can be drawn through our points of observation.</description>
    </item>
    
    <item>
      <title>Why is OLS BLUE?</title>
      <link>https://NathanielF.github.io/portfolio/ols-and-the-gauss-markov-theorem/</link>
      <pubDate>Sun, 06 Nov 2016 13:00:25 +0530</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/ols-and-the-gauss-markov-theorem/</guid>
      <description>There is an aesthetic discipline to mathematics as much as there is an expectation of rigour. We shall attempt to showcase the relation between aesthetic dimension of a proof with its explanatory purchase. We shall take as our example a standard proof used in regression analysis to find the slope of a line/plane which maps the trajectory of a series of observed points/locations along a trajectory. Given this problem we can draw a line/plane with the slope and it can be shown that this line (as a function of slope) is the best available line that can be drawn through our points of observation.</description>
    </item>
    
    <item>
      <title>Conditional Probability and Monty Hall</title>
      <link>https://NathanielF.github.io/blog/montyhall/</link>
      <pubDate>Sun, 08 May 2016 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/blog/montyhall/</guid>
      <description>Probability is difficult because it is hard to properly model the relationships between events, and the facts even when recognised are unintuitive. Conditional probability is a tool for assessing the probability of a hypothesis given some evidence. To represent this mathematically we have to distinguish over all possible events, and assess the frequency of when the hypothesis is confirmed within the subset of possible worlds where our evidence is deemed to have occurred.</description>
    </item>
    
    <item>
      <title>Fisher&#39;s Exact P-values</title>
      <link>https://NathanielF.github.io/portfolio/causation/causal_models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/causation/causal_models/</guid>
      <description>In this post we outline an attempt to pin down causal relationships in social science using Fisher’s approach of exact p-values
Fisher’s Exact p-values and Randomised distributions The fundamental problem of causal inference is best illustrated within the ideal of a randomised experiment where we can observe for each patient their previous traits, whether or not they were treated and their individual outcomes under a given treatment plan.
It is easy to see that we are unable to observe the counterfactual outcome for each particular patient under this treatment plan.</description>
    </item>
    
  </channel>
</rss>