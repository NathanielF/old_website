<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Portfolios on Randomly Jittered</title>
    <link>https://NathanielF.github.io/portfolio/</link>
    <description>Recent content in Portfolios on Randomly Jittered</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2017 Nathaniel Forde</copyright>
    <lastBuildDate>Fri, 26 May 2017 08:22:07 +0100</lastBuildDate>
    
	<atom:link href="https://NathanielF.github.io/portfolio/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Neural Networks</title>
      <link>https://NathanielF.github.io/portfolio/neuralnet/</link>
      <pubDate>Fri, 26 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/neuralnet/</guid>
      <description>The clear limitations of the perceptron prompted criticism of the connectionist paradigm in machine learning. In 1969 Marvin Minsky released a scathing critique of the prospects for this approah elaborating a series of examples which could not (even in principle) be learned by the perceptron. Notably the concept of exclusive disjunction could not be captured by any single perceptron.
In part this limitation stems from the fact that the Heaviside activation function forces us to have a discrete delta between our prediction and target.</description>
    </item>
    
    <item>
      <title>The KNN algorithm</title>
      <link>https://NathanielF.github.io/portfolio/knn/</link>
      <pubDate>Fri, 26 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/knn/</guid>
      <description>In this post we will implement the K-Nearest Neighbour classification algorithm. The idea is simply stated that &amp;ldquo;we are the company we keep&amp;rdquo;. The algorithm surveys an entire known population and compares each candidate to that population to determine where (along a series of metrics) that candidate best fits. Once we have ascertained a measure of fit we further identify the candidate with the most common class of the members of the population nearest to them in the population.</description>
    </item>
    
    <item>
      <title>Decision Boundaries</title>
      <link>https://NathanielF.github.io/portfolio/perceptron/decisionpoints/</link>
      <pubDate>Mon, 08 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/perceptron/decisionpoints/</guid>
      <description>Despite sounding like the protagonist of rejected sci-fi script, the perceptron is just a lowly algorithm. Inspired by the model of neuronal triggering patterns, this simple linear classifier was designed to enact a pseudo-biological processing mechanism. As our brain receives and processes different inputs it unconsciously corrects for infelicities in rendering based on a weighting derived from past experience. The thought is that we are progressively conditioned for behaviour that expedites performance of perceptual and physical reflex.</description>
    </item>
    
    <item>
      <title>Matrices and Machine Learning</title>
      <link>https://NathanielF.github.io/portfolio/matrixmultiplication/</link>
      <pubDate>Mon, 08 May 2017 08:22:07 +0100</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/matrixmultiplication/</guid>
      <description>The ubiquity of matrix manipulation in machine learning means that languages like R and Python, which have been optimised for linear algebra, are very well suited to the task of making robots think. It is less clear to me whether Skynet could be written in Golang?
There are a number of machine learning packages in development but none have the naturalness of an R implementation. To that end I decided to create some of the functionality I need.</description>
    </item>
    
    <item>
      <title>Why is OLS BLUE?</title>
      <link>https://NathanielF.github.io/portfolio/ols-and-the-gauss-markov-theorem/</link>
      <pubDate>Sun, 06 Nov 2016 13:00:25 +0530</pubDate>
      
      <guid>https://NathanielF.github.io/portfolio/ols-and-the-gauss-markov-theorem/</guid>
      <description>There is an aesthetic discipline to mathematics as much as there is an expectation of rigour. We shall attempt to showcase the relation between aesthetic dimension of a proof with its explanatory purchase. We shall take as our example a standard proof used in regression analysis to find the slope of a line/plane which maps the trajectory of a series of observed points/locations along a trajectory. Given this problem we can draw a line/plane with the slope and it can be shown that this line (as a function of slope) is the best available line that can be drawn through our points of observation.</description>
    </item>
    
  </channel>
</rss>